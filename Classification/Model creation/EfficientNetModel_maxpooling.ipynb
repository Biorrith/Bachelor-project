{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "410a8495-5158-4aed-99ea-a1479a9406d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import io\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from tensorflow.keras.applications import *\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(tf.keras.__version__)\n",
    "# for subdir in sorted(os.listdir()):\n",
    "#     print(subdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb5b9dff-ef62-4be4-9aa7-0f8e291f99c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE THE EFFICIENTNET MODEL\n",
    "\n",
    "def createEfficientNet(input_shape, numb, trainable):\n",
    "    \n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "        print(\"Running on TPU \", tpu.cluster_spec().as_dict()[\"worker\"])\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    except ValueError:\n",
    "        print(\"Not connected to a TPU runtime. Using CPU/GPU strategy\")\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "    with strategy.scope():   # loading pretrained conv base model\n",
    "    \n",
    "        \n",
    "        conv_base = EfficientNetB7(weights=\"imagenet\", include_top=False, input_shape=input_shape)    \n",
    "\n",
    "        #for layer in conv_base.layers:\n",
    "        #    layer.trainable = trainable        \n",
    "        conv_base.trainable = trainable\n",
    "\n",
    "        dropout_rate = 0.2\n",
    "        model = models.Sequential()\n",
    "\n",
    "        model.add(conv_base)\n",
    "        model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
    "        #model.add(layers.Flatten(name=\"flatten\"))\n",
    "        if dropout_rate > 0:\n",
    "            model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
    "        #model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
    "        model.add(layers.Dense(number_of_classes, activation=\"sigmoid\", name=\"fc_out\"))\n",
    "\n",
    "        model.compile(\n",
    "            #optimizer=optimizers.RMSprop(lr=2e-5),a\n",
    "            optimizer=optimizers.Adam(learning_rate=0.0001),\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            metrics=[\"acc\"],\n",
    "        )\n",
    "        #model.compile(\n",
    "        #    optimizer=\"adam\", \n",
    "        #    loss=\"categorical_crossentropy\", \n",
    "        #    metrics=[\"accuracy\"]\n",
    "        #)\n",
    "        model.summary()\n",
    "        \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff671ef-33c8-47c4-84be-25b6f312b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD IN THE DATA\n",
    "\n",
    "def createDataGenerators(data_dir, image_size, batch_size, seed = 1):\n",
    "    \n",
    "    # Train data genrator\n",
    "    # Settings from AMT\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rotation_range = 180,\n",
    "        horizontal_flip = True,\n",
    "        vertical_flip = True,\n",
    "        zoom_range=0.3,\n",
    "        validation_split=0.2,\n",
    "        brightness_range=[0.9, 1.1]\n",
    "    )\n",
    "\n",
    "    # Note that the validation data should not be augmented!\n",
    "    test_datagen = ImageDataGenerator(\n",
    "        validation_split=0.2\n",
    "    )\n",
    "    \n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        data_dir,\n",
    "        # All images will be resized to target height and width.\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=batch_size,\n",
    "        # Since we use categorical_crossentropy loss, we need categorical labels\n",
    "        class_mode=\"categorical\",\n",
    "        subset='training',\n",
    "        shuffle=True,\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        # All images will be resized to target height and width.\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=\"categorical\",\n",
    "        subset='validation',\n",
    "        shuffle=False,\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    return train_generator, validation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3789019e-54ce-4364-9ac5-8f7bebdd27f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not connected to a TPU runtime. Using CPU/GPU strategy\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " efficientnetb7 (Functional)  (None, 7, 7, 2560)       64097687  \n",
      "                                                                 \n",
      " gap (GlobalMaxPooling2D)    (None, 2560)              0         \n",
      "                                                                 \n",
      " dropout_out (Dropout)       (None, 2560)              0         \n",
      "                                                                 \n",
      " fc_out (Dense)              (None, 2)                 5122      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 64,102,809\n",
      "Trainable params: 5,122\n",
      "Non-trainable params: 64,097,687\n",
      "_________________________________________________________________\n",
      "Found 488 images belonging to 2 classes.\n",
      "Found 121 images belonging to 2 classes.\n",
      "DET ER HER: 16\n",
      "Epoch 1/58\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018EC308EC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018EC308EC18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.9578 - acc: 0.6667WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018E05D4A5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018E05D4A5E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "15/15 [==============================] - 105s 5s/step - loss: 0.9578 - acc: 0.6667 - val_loss: 0.1014 - val_acc: 0.9688\n",
      "Epoch 2/58\n",
      "15/15 [==============================] - 86s 6s/step - loss: 0.7779 - acc: 0.6952 - val_loss: 0.0771 - val_acc: 0.9688\n",
      "Epoch 3/58\n",
      "15/15 [==============================] - 83s 6s/step - loss: 0.5625 - acc: 0.7961 - val_loss: 0.0606 - val_acc: 0.9688\n",
      "Epoch 4/58\n",
      "15/15 [==============================] - 82s 6s/step - loss: 0.5525 - acc: 0.7939 - val_loss: 0.0534 - val_acc: 0.9688\n",
      "Epoch 5/58\n",
      "15/15 [==============================] - 79s 5s/step - loss: 0.4966 - acc: 0.8026 - val_loss: 0.0460 - val_acc: 0.9792\n",
      "Epoch 6/58\n",
      "15/15 [==============================] - 78s 5s/step - loss: 0.4409 - acc: 0.8355 - val_loss: 0.0477 - val_acc: 0.9688\n",
      "Epoch 7/58\n",
      "15/15 [==============================] - 78s 5s/step - loss: 0.3993 - acc: 0.8399 - val_loss: 0.0357 - val_acc: 0.9792\n",
      "Epoch 8/58\n",
      "15/15 [==============================] - 79s 5s/step - loss: 0.2755 - acc: 0.8969 - val_loss: 0.0376 - val_acc: 0.9792\n",
      "Epoch 9/58\n",
      "15/15 [==============================] - 78s 5s/step - loss: 0.3064 - acc: 0.8816 - val_loss: 0.0308 - val_acc: 1.0000\n",
      "Epoch 10/58\n",
      "15/15 [==============================] - 78s 5s/step - loss: 0.3347 - acc: 0.8662 - val_loss: 0.0302 - val_acc: 0.9896\n",
      "Epoch 11/58\n",
      "15/15 [==============================] - 78s 5s/step - loss: 0.2543 - acc: 0.9079 - val_loss: 0.0384 - val_acc: 0.9688\n",
      "Epoch 12/58\n",
      "15/15 [==============================] - 77s 5s/step - loss: 0.2886 - acc: 0.8816 - val_loss: 0.0271 - val_acc: 0.9896\n",
      "Epoch 13/58\n",
      "15/15 [==============================] - 77s 5s/step - loss: 0.3392 - acc: 0.8772 - val_loss: 0.0278 - val_acc: 0.9896\n",
      "Epoch 14/58\n",
      "15/15 [==============================] - 78s 5s/step - loss: 0.3455 - acc: 0.8706 - val_loss: 0.0247 - val_acc: 1.0000\n",
      "Epoch 15/58\n",
      "15/15 [==============================] - 78s 5s/step - loss: 0.2131 - acc: 0.9145 - val_loss: 0.0239 - val_acc: 0.9896\n",
      "Epoch 16/58\n",
      "15/15 [==============================] - 80s 5s/step - loss: 0.2565 - acc: 0.9145 - val_loss: 0.0468 - val_acc: 0.9688\n",
      "Epoch 17/58\n",
      "15/15 [==============================] - 80s 5s/step - loss: 0.2368 - acc: 0.9189 - val_loss: 0.0228 - val_acc: 0.9896\n",
      "Epoch 18/58\n",
      "15/15 [==============================] - 82s 6s/step - loss: 0.2597 - acc: 0.9146 - val_loss: 0.0238 - val_acc: 0.9896\n",
      "Epoch 19/58\n",
      "15/15 [==============================] - 81s 5s/step - loss: 0.2212 - acc: 0.9232 - val_loss: 0.0231 - val_acc: 0.9896\n",
      "Epoch 20/58\n",
      "15/15 [==============================] - 80s 5s/step - loss: 0.2018 - acc: 0.9276 - val_loss: 0.0209 - val_acc: 1.0000\n",
      "Epoch 21/58\n",
      "15/15 [==============================] - 81s 5s/step - loss: 0.1840 - acc: 0.9386 - val_loss: 0.0206 - val_acc: 0.9896\n",
      "Epoch 22/58\n",
      "15/15 [==============================] - 80s 5s/step - loss: 0.2592 - acc: 0.9079 - val_loss: 0.0202 - val_acc: 0.9896\n",
      "Epoch 23/58\n",
      "15/15 [==============================] - 82s 5s/step - loss: 0.1948 - acc: 0.9211 - val_loss: 0.0205 - val_acc: 0.9896\n",
      "Epoch 24/58\n",
      "15/15 [==============================] - 79s 5s/step - loss: 0.1892 - acc: 0.9167 - val_loss: 0.0197 - val_acc: 1.0000\n",
      "Epoch 25/58\n",
      "15/15 [==============================] - 80s 5s/step - loss: 0.2168 - acc: 0.9167 - val_loss: 0.0190 - val_acc: 0.9896\n",
      "Epoch 26/58\n",
      "15/15 [==============================] - 80s 5s/step - loss: 0.2653 - acc: 0.9101 - val_loss: 0.0191 - val_acc: 1.0000\n",
      "Epoch 27/58\n",
      "15/15 [==============================] - 78s 5s/step - loss: 0.1741 - acc: 0.9408 - val_loss: 0.0187 - val_acc: 0.9896\n",
      "Epoch 28/58\n",
      "15/15 [==============================] - 80s 5s/step - loss: 0.2281 - acc: 0.9145 - val_loss: 0.0276 - val_acc: 0.9896\n",
      "Epoch 29/58\n",
      "15/15 [==============================] - 79s 5s/step - loss: 0.1667 - acc: 0.9430 - val_loss: 0.0217 - val_acc: 0.9896\n",
      "Epoch 30/58\n",
      "15/15 [==============================] - 80s 5s/step - loss: 0.1804 - acc: 0.9364 - val_loss: 0.0192 - val_acc: 1.0000\n",
      "Epoch 31/58\n",
      "15/15 [==============================] - 80s 5s/step - loss: 0.1573 - acc: 0.9496 - val_loss: 0.0173 - val_acc: 0.9896\n",
      "Epoch 32/58\n",
      "15/15 [==============================] - 79s 5s/step - loss: 0.1914 - acc: 0.9189 - val_loss: 0.0198 - val_acc: 0.9896\n",
      "Epoch 33/58\n",
      "15/15 [==============================] - 79s 5s/step - loss: 0.1892 - acc: 0.9430 - val_loss: 0.0188 - val_acc: 0.9896\n",
      "Epoch 34/58\n",
      "15/15 [==============================] - 80s 5s/step - loss: 0.1809 - acc: 0.9276 - val_loss: 0.0177 - val_acc: 1.0000\n",
      "Epoch 35/58\n",
      "15/15 [==============================] - 80s 6s/step - loss: 0.1605 - acc: 0.9452 - val_loss: 0.0187 - val_acc: 0.9896\n",
      "Epoch 36/58\n",
      "15/15 [==============================] - 79s 5s/step - loss: 0.1944 - acc: 0.9189 - val_loss: 0.0174 - val_acc: 0.9896\n",
      "Epoch 37/58\n",
      "15/15 [==============================] - 81s 5s/step - loss: 0.1644 - acc: 0.9408 - val_loss: 0.0173 - val_acc: 0.9896\n",
      "Epoch 38/58\n",
      "15/15 [==============================] - 79s 5s/step - loss: 0.1550 - acc: 0.9452 - val_loss: 0.0174 - val_acc: 0.9896\n",
      "Epoch 39/58\n",
      "15/15 [==============================] - 76s 5s/step - loss: 0.1385 - acc: 0.9386 - val_loss: 0.0176 - val_acc: 0.9896\n",
      "Epoch 40/58\n",
      "15/15 [==============================] - 75s 5s/step - loss: 0.1595 - acc: 0.9474 - val_loss: 0.0177 - val_acc: 0.9896\n",
      "Epoch 41/58\n",
      "15/15 [==============================] - 76s 5s/step - loss: 0.1403 - acc: 0.9474 - val_loss: 0.0175 - val_acc: 0.9896\n",
      "Epoch 42/58\n",
      "15/15 [==============================] - 79s 5s/step - loss: 0.1071 - acc: 0.9671 - val_loss: 0.0176 - val_acc: 0.9896\n",
      "Epoch 43/58\n",
      "15/15 [==============================] - 77s 5s/step - loss: 0.1286 - acc: 0.9474 - val_loss: 0.0196 - val_acc: 0.9896\n",
      "Epoch 44/58\n",
      "15/15 [==============================] - 77s 5s/step - loss: 0.1257 - acc: 0.9561 - val_loss: 0.0171 - val_acc: 0.9896\n",
      "Epoch 45/58\n",
      "15/15 [==============================] - 81s 5s/step - loss: 0.2041 - acc: 0.9342 - val_loss: 0.0189 - val_acc: 0.9896\n",
      "Epoch 46/58\n",
      "15/15 [==============================] - 78s 5s/step - loss: 0.1579 - acc: 0.9386 - val_loss: 0.0201 - val_acc: 0.9896\n",
      "Epoch 47/58\n",
      "15/15 [==============================] - 78s 5s/step - loss: 0.1231 - acc: 0.9518 - val_loss: 0.0182 - val_acc: 1.0000\n",
      "Epoch 48/58\n",
      "15/15 [==============================] - 74s 5s/step - loss: 0.1486 - acc: 0.9496 - val_loss: 0.0222 - val_acc: 0.9896\n",
      "Epoch 49/58\n",
      "15/15 [==============================] - 77s 5s/step - loss: 0.1302 - acc: 0.9500 - val_loss: 0.0177 - val_acc: 0.9896\n",
      "Epoch 50/58\n",
      "15/15 [==============================] - 78s 5s/step - loss: 0.1336 - acc: 0.9496 - val_loss: 0.0206 - val_acc: 0.9896\n",
      "Epoch 51/58\n",
      "15/15 [==============================] - 77s 5s/step - loss: 0.1855 - acc: 0.9430 - val_loss: 0.0191 - val_acc: 0.9896\n",
      "Epoch 52/58\n",
      "15/15 [==============================] - 76s 5s/step - loss: 0.1449 - acc: 0.9430 - val_loss: 0.0168 - val_acc: 0.9896\n",
      "Epoch 53/58\n",
      "15/15 [==============================] - 77s 5s/step - loss: 0.1009 - acc: 0.9649 - val_loss: 0.0192 - val_acc: 0.9896\n",
      "Epoch 54/58\n",
      "15/15 [==============================] - 76s 5s/step - loss: 0.0907 - acc: 0.9583 - val_loss: 0.0202 - val_acc: 0.9896\n",
      "Epoch 55/58\n",
      "15/15 [==============================] - 77s 5s/step - loss: 0.1374 - acc: 0.9518 - val_loss: 0.0160 - val_acc: 0.9896\n",
      "Epoch 56/58\n",
      "15/15 [==============================] - 75s 5s/step - loss: 0.1269 - acc: 0.9430 - val_loss: 0.0186 - val_acc: 0.9896\n",
      "Epoch 57/58\n",
      "15/15 [==============================] - 78s 5s/step - loss: 0.1590 - acc: 0.9417 - val_loss: 0.0161 - val_acc: 0.9896\n",
      "Epoch 58/58\n",
      "15/15 [==============================] - 76s 5s/step - loss: 0.1181 - acc: 0.9539 - val_loss: 0.0181 - val_acc: 0.9896\n",
      "Model predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dbior\\anaconda3\\envs\\bs\\lib\\site-packages\\ipykernel_launcher.py:54: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000018E7F59C438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000018E7F59C438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98        91\n",
      "           1       0.97      0.93      0.95        30\n",
      "\n",
      "    accuracy                           0.98       121\n",
      "   macro avg       0.97      0.96      0.97       121\n",
      "weighted avg       0.98      0.98      0.98       121\n",
      "\n",
      "F1-score: 0.975064239607421\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#RUN THE CODE, BROTHER!\n",
    "\n",
    "if __name__=='__main__': \n",
    "    \n",
    "    data_dir = 'data/train'\n",
    "    \n",
    "    # Directory for saving h5 models for each run\n",
    "    models_dir = './models_save'   \n",
    "    log_dir = './hparam_tuning'\n",
    "\n",
    "    number_of_classes = 2\n",
    "    batch_size = 32\n",
    "    epochs = 58 # 38 EfficientNetB0+others, 27 MobileNetV2, 25 InceptionV3, 35 ResNet50V2, 25 ResNet50V2 with imagenet\n",
    "    image_size = 224 # MobileNetV2, EfficientNetB0, ResNet50V2\n",
    "\n",
    "    NUM_DATA = 609\n",
    "    TEST_SPLIT = 0.2\n",
    "    NUM_TRAIN = NUM_DATA*(1.0 - TEST_SPLIT)\n",
    "    NUM_TEST = NUM_DATA*TEST_SPLIT\n",
    "\n",
    "    input_shape= (image_size, image_size, 3)\n",
    "\n",
    "    model = createEfficientNet(input_shape, number_of_classes, False)\n",
    "\n",
    "    train_generator, validation_generator = createDataGenerators(data_dir, image_size, batch_size)\n",
    "\n",
    "    print(\"DET ER HER: \" + str(train_generator.__len__()))\n",
    "    \n",
    "    # Extend with examed hyperparameters\n",
    "    HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([32]))\n",
    "    HP_IMG_SIZE = hp.HParam('image_size', hp.Discrete([224]))\n",
    "\n",
    "    hparams = {\n",
    "            HP_BATCH_SIZE: batch_size,\n",
    "            HP_IMG_SIZE: image_size\n",
    "            }\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch= NUM_TRAIN // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=NUM_TEST // batch_size,\n",
    "        verbose=1,\n",
    "        callbacks=[\n",
    "                   tf.keras.callbacks.TensorBoard(log_dir),\n",
    "                   hp.KerasCallback(log_dir, hparams),\n",
    "                   ],\n",
    "    )\n",
    "    \n",
    "    print('Model predict')\n",
    "    Y_pred = model.predict_generator(validation_generator) #, 173//batch_size+1\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "    print('Confusion Matrix')\n",
    "    print(classification_report(validation_generator.classes, y_pred))\n",
    "    report = classification_report(validation_generator.classes, y_pred, output_dict=True)\n",
    "    f1_score = report['weighted avg']['f1-score']\n",
    "    print('F1-score:', f1_score)\n",
    "\n",
    "\n",
    "    model.save(models_dir + '/' +  'EfficientNetB7.h5')\n",
    "\n",
    "    conf = confusion_matrix(validation_generator.classes, y_pred, normalize='true')\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(conf, annot=True, cmap=plt.cm.Blues)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig('confmatrix.png')\n",
    "    plt.show()\n",
    "    plt.close(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fead0b25-e62b-4b52-bf58-f435d2734569",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
